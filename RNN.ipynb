{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMljB9Uge8GpUzE9Hs+AfVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuananht/long-short-term-memory/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIng5uTFfTSW",
        "outputId": "e986352c-c09e-42c7-d387-081c13539f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X2mKH-teqnF",
        "outputId": "d2799228-cb32-413f-d141-20018acc7f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "vocabulary_size = 8000\n",
        "unknow_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "print(\"Reading CSV file...\")\n",
        "\n",
        "csv_file_path = \"/content/drive/My Drive/UIT-GRAD/Model/comments/reddit-comments-2015-08.csv\"\n",
        "\n",
        "with open(csv_file_path, 'r') as f:\n",
        "  reader = csv.reader(f, skipinitialspace=True)\n",
        "  next(reader)\n",
        "\n",
        "  sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "  sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "\n",
        "print(\"Parsed %d sentences\" % (len(sentences)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Reading CSV file...\n",
            "Parsed 79170 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt-Mb3q3h9qr",
        "outputId": "3eaf5561-0aef-4e15-9624-c6f464ee767e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens\" % len(word_freq.items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 65499 unique words tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw4zDnQNfSlU",
        "outputId": "6712b5e7-7a74-4320-cbd7-5df9d3994fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "vocab = word_freq.most_common(vocabulary_size - 1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknow_token)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])\n",
        "\n",
        "print(\"vocabulary size\", vocabulary_size)\n",
        "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size 8000\n",
            "The least frequent word in our vocabulary is 'documentary' and appeared 10 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQnoIG-8i6Fe",
        "outputId": "b8e64e70-0f2b-4fd3-84d9-12fdb9b607ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for i, sent in enumerate(tokenized_sentences):\n",
        "  tokenized_sentences[i] = [w if w in word_to_index else unknow_token for w in sent]\n",
        "\n",
        "print(\"example sentence\", sentences[0])\n",
        "print(\"example sentence after pre-processing\", tokenized_sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example sentence SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END\n",
            "example sentence after pre-processing ['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27wV-q6gjY-R",
        "outputId": "4757dd19-3111-4a73-ad00-cb3c90a83ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
        "\n",
        "print(\"x_train\", x_train)\n",
        "print(\"y_train\", y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train [list([0, 6, 3494, 7, 155, 795, 25, 222, 8, 32, 20, 202, 4954, 350, 91, 6, 66, 207, 5, 2])\n",
            " list([0, 11, 17, 7, 3094, 5974, 7999, 7999, 5974, 2])\n",
            " list([0, 988, 1478, 226, 597, 15, 776, 3410, 2957, 4, 7999, 597, 471, 5975, 4, 491, 597, 471, 5976, 2702, 4, 8, 71, 5681, 15, 7999, 7999, 2])\n",
            " ...\n",
            " list([0, 7999, 4, 41, 7999, 4, 13, 63, 9, 152, 757, 7999, 57, 3, 7999, 12, 97, 16, 619, 67, 11, 109, 20, 2])\n",
            " list([0, 38, 144, 3585, 24, 7999, 7999, 7999, 8, 1052, 564, 7999, 7999, 7999, 7999, 2])\n",
            " list([0, 3, 4287, 19, 7999, 18, 174, 12, 232, 74, 101, 1292, 14, 24, 161, 8, 12, 6, 160, 16, 131, 3, 564, 68, 11, 17, 790, 5, 26, 7999, 2])]\n",
            "y_train [list([6, 3494, 7, 155, 795, 25, 222, 8, 32, 20, 202, 4954, 350, 91, 6, 66, 207, 5, 2, 1])\n",
            " list([11, 17, 7, 3094, 5974, 7999, 7999, 5974, 2, 1])\n",
            " list([988, 1478, 226, 597, 15, 776, 3410, 2957, 4, 7999, 597, 471, 5975, 4, 491, 597, 471, 5976, 2702, 4, 8, 71, 5681, 15, 7999, 7999, 2, 1])\n",
            " ...\n",
            " list([7999, 4, 41, 7999, 4, 13, 63, 9, 152, 757, 7999, 57, 3, 7999, 12, 97, 16, 619, 67, 11, 109, 20, 2, 1])\n",
            " list([38, 144, 3585, 24, 7999, 7999, 7999, 8, 1052, 564, 7999, 7999, 7999, 7999, 2, 1])\n",
            " list([3, 4287, 19, 7999, 18, 174, 12, 232, 74, 101, 1292, 14, 24, 161, 8, 12, 6, 160, 16, 131, 3, 564, 68, 11, 17, 790, 5, 26, 7999, 2, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cllZB743UE4c"
      },
      "source": [
        "class RNNNumpy:\n",
        "  def __init__(self, word_dim, hidden_dim=100, bptt_trucate=4):\n",
        "    self.word_dim = word_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.bptt_trucate = bptt_trucate\n",
        "\n",
        "    self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "    self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "    self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "\n",
        "  def forward_propagation(self, x):\n",
        "    T = len(x)\n",
        "\n",
        "    s = np.zeros((T + 1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "\n",
        "    for t in np.arange(T):\n",
        "      s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
        "      o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s]\n",
        "\n",
        "  def predict(self, x):\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27_kdzwVg8A"
      },
      "source": [
        "RNNNumpy.forward_propagation = forward_propagation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l4_6eamVnNz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBW0kx0KVtvS"
      },
      "source": [
        "RNNNumpy.predict = predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_qTqjBfVxRr",
        "outputId": "0ef058b4-b51e-4dd9-e784-d4d1bddc09c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "o, s = model.forward_propagation(x_train[10])\n",
        "print(o.shape)\n",
        "print(o)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3c3fe552c1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-8c8fe5c8cd62>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
          ]
        }
      ]
    }
  ]
}