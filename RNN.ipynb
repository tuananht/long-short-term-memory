{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpDnHq62sPcRgIJW3llvbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuananht/long-short-term-memory/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIng5uTFfTSW",
        "outputId": "83cb306c-e81a-4e32-c386-c0cb85019c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X2mKH-teqnF",
        "outputId": "0bea297b-7020-4f74-f95c-4063d34bd078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "vocabulary_size = 8000\n",
        "unknow_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "print(\"Reading CSV file...\")\n",
        "\n",
        "csv_file_path = \"/content/drive/My Drive/UIT-GRAD/Model/comments/reddit-comments-2015-08.csv\"\n",
        "\n",
        "with open(csv_file_path, 'r') as f:\n",
        "  reader = csv.reader(f, skipinitialspace=True)\n",
        "  next(reader)\n",
        "\n",
        "  sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "  sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "\n",
        "print(\"Parsed %d sentences\" % (len(sentences)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Reading CSV file...\n",
            "Parsed 79170 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt-Mb3q3h9qr",
        "outputId": "c642ef7b-324a-41cc-d6f6-cb0e209fb791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens\" % len(word_freq.items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 65499 unique words tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw4zDnQNfSlU",
        "outputId": "7a3f546a-da57-4bef-db92-f7eacda580b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "vocab = word_freq.most_common(vocabulary_size - 1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknow_token)\n",
        "word_to_index = dict([(w,i) for i, w in enumerate(index_to_word)])\n",
        "\n",
        "print(\"vocabulary size\", vocabulary_size)\n",
        "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size 8000\n",
            "The least frequent word in our vocabulary is 'documentary' and appeared 10 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQnoIG-8i6Fe",
        "outputId": "5a253281-cbc4-4262-b098-6146f329e94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for i, sent in enumerate(tokenized_sentences):\n",
        "  tokenized_sentences[i] = [w if w in word_to_index else unknow_token for w in sent]\n",
        "\n",
        "print(\"example sentence\", sentences[0])\n",
        "print(\"example sentence after pre-processing\", tokenized_sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example sentence SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END\n",
            "example sentence after pre-processing ['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27wV-q6gjY-R",
        "outputId": "a2f4f4dd-534c-4c27-8866-3f17608b8f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
        "\n",
        "print(\"x_train\", x_train)\n",
        "print(\"y_train\", y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train [list([0, 6, 3494, 7, 155, 795, 25, 222, 8, 32, 20, 202, 4954, 350, 91, 6, 66, 207, 5, 2])\n",
            " list([0, 11, 17, 7, 3094, 5974, 7999, 7999, 5974, 2])\n",
            " list([0, 988, 1478, 226, 597, 15, 776, 3410, 2957, 4, 7999, 597, 471, 5975, 4, 491, 597, 471, 5976, 2702, 4, 8, 71, 5681, 15, 7999, 7999, 2])\n",
            " ...\n",
            " list([0, 7999, 4, 41, 7999, 4, 13, 63, 9, 152, 757, 7999, 57, 3, 7999, 12, 97, 16, 619, 67, 11, 109, 20, 2])\n",
            " list([0, 38, 144, 3585, 24, 7999, 7999, 7999, 8, 1052, 564, 7999, 7999, 7999, 7999, 2])\n",
            " list([0, 3, 4287, 19, 7999, 18, 174, 12, 232, 74, 101, 1292, 14, 24, 161, 8, 12, 6, 160, 16, 131, 3, 564, 68, 11, 17, 790, 5, 26, 7999, 2])]\n",
            "y_train [list([6, 3494, 7, 155, 795, 25, 222, 8, 32, 20, 202, 4954, 350, 91, 6, 66, 207, 5, 2, 1])\n",
            " list([11, 17, 7, 3094, 5974, 7999, 7999, 5974, 2, 1])\n",
            " list([988, 1478, 226, 597, 15, 776, 3410, 2957, 4, 7999, 597, 471, 5975, 4, 491, 597, 471, 5976, 2702, 4, 8, 71, 5681, 15, 7999, 7999, 2, 1])\n",
            " ...\n",
            " list([7999, 4, 41, 7999, 4, 13, 63, 9, 152, 757, 7999, 57, 3, 7999, 12, 97, 16, 619, 67, 11, 109, 20, 2, 1])\n",
            " list([38, 144, 3585, 24, 7999, 7999, 7999, 8, 1052, 564, 7999, 7999, 7999, 7999, 2, 1])\n",
            " list([3, 4287, 19, 7999, 18, 174, 12, 232, 74, 101, 1292, 14, 24, 161, 8, 12, 6, 160, 16, 131, 3, 564, 68, 11, 17, 790, 5, 26, 7999, 2, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}